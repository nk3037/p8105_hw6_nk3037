---
title: "P8105_hw6_nk3037"
author: "Navya Koneripalli"
date: "2023-11-28"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(broom)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Question 2
### Setup
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

### Bootstrapping
```{r}
# Initial linear model
lin_model = lm(tmax ~ tmin + prcp, data = weather_df)

# Bootstrap sample
num_samples = 5000

# Creating a matrix for the estimates
bootstrap_estimates = matrix(NA, nrow = num_samples, ncol = 2)

# Bootstrapping
for (i in 1:num_samples) {
  bootstrap_sample = weather_df[sample(nrow(weather_df), replace = TRUE), ]
  bootstrap_model = lm(tmax ~ tmin + prcp, data = bootstrap_sample)
  bootstrap_estimates[i, 1] = broom::glance(bootstrap_model)$r.squared
  bootstrap_estimates[i, 2] = sum(broom::tidy(bootstrap_model)$estimate[-1])
}

# Plotting the distribution
hist(bootstrap_estimates[, 1], main = "Distribution of r^2", xlab = "r^2")
```
The distribution of r squared is skewed left. The most common r^2 value is between 0.92-0.925. The mean r^2 value is 0.92. Since the distribution is not normal, it may be safe to assume that a linear model is not the best approach for this data and instead a generalized linear model or non-parametric methods may be better.

```{r}
hist(bootstrap_estimates[, 2], main = "Distribution of log(hat(β1) * hat(β2))", xlab = "log(β1*β2)")
```

The distribution of log (b1*b2) is normally distributed. The mean and mode is between 1.01-1.02. Based on the Central Limit Theorem, it is expected that the distribution across 5000 samples is approximately normal. Since the product of the log of b1 and b2 is normal, we can conclude that b1 and b2 are also log-normally distributed.

### Confidence interval
```{r}
quantiles <- quantile(bootstrap_estimates, c(0.025, 0.975), na.rm = TRUE)

lower_bound_r_squared = quantiles["2.5%"]
upper_bound_r_squared = quantiles["97.5%"]

lower_bound_log_beta = quantiles["2.5%"]
upper_bound_log_beta = quantiles["97.5%"]

cat("95% CI for r^2:", lower_bound_r_squared, "-", upper_bound_r_squared, "\n")
cat("95% CI for log(β̂1∗β̂2):", lower_bound_log_beta, "-", upper_bound_log_beta, "\n")
```


## Question 3
```{r}
# Loading the birth weight data
birthweight = read_csv("./data/birthweight.csv")

# Data cleaning
birthweight = janitor::clean_names(birthweight, case = "snake")%>% 
  mutate(
    babysex = factor(babysex),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9)),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8)),
    malform = factor(malform),
    smoken = factor(smoken),
    parity = factor(parity)
  ) %>% 
  na.omit(birthweight)

# Regression model
model = lm (bwt ~ mheight + delwt, data = birthweight)
summary(model)

# Adding in predictions and residuals
birthweight = cbind(birthweight, predict(model, interval = "prediction"))
colnames(birthweight)[ncol(birthweight) - 2] = "predictions"
colnames(birthweight)[ncol(birthweight) - 1] = "lower_bound"
colnames(birthweight)[ncol(birthweight)] = "upper_bound"

# Plot residuals against fitted values
plot(x = predict(model), y = residuals(model),
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values",
     col = "black", pch = 1)
abline(h = 0, col = "red", lty = 2)
```

The simple linear model examines the effect that family income and parents' race (mother and father race) have on birth weight. Based on the fitted values vs predicted values plot, since the points are clustered symmetriclly around 0, we can conclude that the variability of the residuals varies across different levels of the fitted values. This indicates heteroscedasticity. This means the relationship between predictors and the respone variable is not linear.
