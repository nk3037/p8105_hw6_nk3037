---
title: "P8105_hw6_nk3037"
author: "Navya Koneripalli"
date: "2023-11-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(broom)
library(modelr)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Question 2
### Setup
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

### Bootstrapping
```{r}
# Initial linear model
lin_model = lm(tmax ~ tmin + prcp, data = weather_df)

# Bootstrap sample
num_samples = 5000

# Creating a matrix for the estimates
bootstrap_estimates = matrix(NA, nrow = num_samples, ncol = 2)

# Bootstrapping
for (i in 1:num_samples) {
  bootstrap_sample = weather_df[sample(nrow(weather_df), replace = TRUE), ]
  bootstrap_model = lm(tmax ~ tmin + prcp, data = bootstrap_sample)
  bootstrap_estimates[i, 1] = broom::glance(bootstrap_model)$r.squared
  bootstrap_estimates[i, 2] = sum(broom::tidy(bootstrap_model)$estimate[-1])
}

# Plotting the distribution
hist(bootstrap_estimates[, 1], main = "Distribution of r^2", xlab = "r^2")
```
The distribution of r squared is skewed left. The most common r^2 value is between 0.92-0.925. The mean r^2 value is 0.92. Since the distribution is not normal, it may be safe to assume that a linear model is not the best approach for this data and instead a generalized linear model or non-parametric methods may be better.

```{r}
hist(bootstrap_estimates[, 2], main = "Distribution of log(hat(β1) * hat(β2))", xlab = "log(β1*β2)")
```

The distribution of log (b1*b2) is normally distributed. The mean and mode is between 1.01-1.02. Based on the Central Limit Theorem, it is expected that the distribution across 5000 samples is approximately normal. Since the product of the log of b1 and b2 is normal, we can conclude that b1 and b2 are also log-normally distributed.

### Confidence interval
```{r}
quantiles <- quantile(bootstrap_estimates, c(0.025, 0.975), na.rm = TRUE)

lower_bound_r_squared = quantiles["2.5%"]
upper_bound_r_squared = quantiles["97.5%"]

lower_bound_log_beta = quantiles["2.5%"]
upper_bound_log_beta = quantiles["97.5%"]

cat("95% CI for r^2:", lower_bound_r_squared, "-", upper_bound_r_squared, "\n")
cat("95% CI for log(β̂1∗β̂2):", lower_bound_log_beta, "-", upper_bound_log_beta, "\n")
```


## Question 3
```{r}
# Loading the birth weight data
birthweight = read_csv("./data/birthweight.csv")

# Data cleaning
birthweight = janitor::clean_names(birthweight, case = "snake")%>% 
  mutate(
    babysex = factor(babysex),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9)),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8)),
    malform = factor(malform),
    smoken = factor(smoken),
    parity = factor(parity)
  ) %>% 
  na.omit(birthweight)

# Regression model
model = lm (bwt ~ mheight + delwt, data = birthweight)
summary(model)
```

The simple linear model examines the effect that mother's height and weight have on birth weight. I think the mother's weight and height are the most accurate predictors for the baby's weight. I think these variables are stronger predictors than income or race/ethnicity, and than the biological determinants of health have a greater impact on birth weight than the environmental and social determinants.

### Model residuals vs fitted values
```{r}
birthweight |> 
  add_predictions(model) |>
  add_residuals(model) |>
  ggplot(aes(x = pred, y = resid)) +  geom_point() +
  geom_smooth(method = "lm") +
  labs(
    title = "Residuals vs Fitted Values ")
```

 Based on the fitted values vs predicted values plot, since the points are clustered symmetriclly around 0, we can conclude that the variability of the residuals varies across different levels of the fitted values. This indicates heteroscedasticity. This means the relationship between predictors and the response variable is not linear.
